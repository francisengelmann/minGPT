{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a character-level language model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    C = CN()\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 3407\n",
    "    C.system.work_dir = './out/chargpt'\n",
    "\n",
    "    # data\n",
    "    C.data = CharDataset.get_default_config()\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-mini'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.block_size = 128\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        self.config = config\n",
    "\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.config.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        # return as tensors\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system:\n",
      "    seed: 3407\n",
      "    work_dir: ./out/chargpt\n",
      "data:\n",
      "    block_size: 128\n",
      "model:\n",
      "    model_type: gpt-mini\n",
      "    n_layer: None\n",
      "    n_head: None\n",
      "    n_embd: None\n",
      "    vocab_size: None\n",
      "    block_size: None\n",
      "    embd_pdrop: 0.1\n",
      "    resid_pdrop: 0.1\n",
      "    attn_pdrop: 0.1\n",
      "trainer:\n",
      "    device: auto\n",
      "    num_workers: 4\n",
      "    max_iters: 1000\n",
      "    batch_size: 64\n",
      "    learning_rate: 0.0005\n",
      "    betas: (0.9, 0.95)\n",
      "    weight_decay: 0.1\n",
      "    grad_norm_clip: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get default config\n",
    "config = get_config()\n",
    "config.trainer.max_iters = 1000\n",
    "print(config)\n",
    "setup_logging(config)\n",
    "set_seed(config.system.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# construct the training dataset\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(config.data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1]), tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
      "        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
      "        44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
      "        54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
      "         1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,  1,\n",
      "        56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58, 53,\n",
      "         1, 42]))\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example training pair\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.71M\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "config.model.block_size = train_dataset.get_block_size()\n",
    "model = GPT(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n",
      "iter_dt 0.00ms; iter 0: train loss 4.18448\n",
      "O God, O God!ie m or rt s o hnetemytosmoetrnmmrermoree oioy nArarA em e meo tnoyn tn tto?n o  omirsttteenrsyisrrsyr ee s ma y nm smoerais mo mattsemne m tstemi son y  m e er ytsnnas oontht mysthno tom met inar oe are at nAre t y ot tyn e yo est mymoynroy  toortht  tty iaryoeasttetnyorr tn soa yryoyoyrnr aytne e t ost tra ya ooeensootrstn eo  are t trrs aynoo mrommee oet aeasie  y meroenaoymeeoo troo tronnromaseyos s e  mye  aoaro erseoth  tyosyst omytot o a e aynrst tmeeaoen ee yoyron e  emsastrt it m tttryt\n",
      "saving model\n",
      "iter_dt 35.15ms; iter 10: train loss 3.02674\n",
      "iter_dt 32.25ms; iter 20: train loss 2.74012\n",
      "iter_dt 32.08ms; iter 30: train loss 2.63531\n",
      "iter_dt 35.18ms; iter 40: train loss 2.57575\n",
      "iter_dt 31.55ms; iter 50: train loss 2.54287\n",
      "iter_dt 32.20ms; iter 60: train loss 2.50715\n",
      "iter_dt 32.22ms; iter 70: train loss 2.50196\n",
      "iter_dt 27.76ms; iter 80: train loss 2.48511\n",
      "iter_dt 30.12ms; iter 90: train loss 2.45725\n",
      "iter_dt 31.72ms; iter 100: train loss 2.50259\n",
      "iter_dt 31.97ms; iter 110: train loss 2.49286\n",
      "iter_dt 32.12ms; iter 120: train loss 2.43652\n",
      "iter_dt 32.01ms; iter 130: train loss 2.46581\n",
      "iter_dt 28.74ms; iter 140: train loss 2.40822\n",
      "iter_dt 32.57ms; iter 150: train loss 2.41939\n",
      "iter_dt 29.28ms; iter 160: train loss 2.41565\n",
      "iter_dt 32.89ms; iter 170: train loss 2.43408\n",
      "iter_dt 32.22ms; iter 180: train loss 2.37739\n",
      "iter_dt 32.20ms; iter 190: train loss 2.38642\n",
      "iter_dt 32.66ms; iter 200: train loss 2.37018\n",
      "iter_dt 32.33ms; iter 210: train loss 2.32281\n",
      "iter_dt 32.23ms; iter 220: train loss 2.28907\n",
      "iter_dt 27.73ms; iter 230: train loss 2.27185\n",
      "iter_dt 33.23ms; iter 240: train loss 2.27026\n",
      "iter_dt 28.36ms; iter 250: train loss 2.23034\n",
      "iter_dt 31.87ms; iter 260: train loss 2.22950\n",
      "iter_dt 31.98ms; iter 270: train loss 2.18221\n",
      "iter_dt 33.06ms; iter 280: train loss 2.17269\n",
      "iter_dt 32.63ms; iter 290: train loss 2.16623\n",
      "iter_dt 32.49ms; iter 300: train loss 2.13883\n",
      "iter_dt 32.37ms; iter 310: train loss 2.15777\n",
      "iter_dt 32.35ms; iter 320: train loss 2.12728\n",
      "iter_dt 31.07ms; iter 330: train loss 2.11286\n",
      "iter_dt 32.57ms; iter 340: train loss 2.09662\n",
      "iter_dt 31.71ms; iter 350: train loss 2.13054\n",
      "iter_dt 29.18ms; iter 360: train loss 2.08888\n",
      "iter_dt 30.82ms; iter 370: train loss 2.09649\n",
      "iter_dt 33.27ms; iter 380: train loss 2.04767\n",
      "iter_dt 32.07ms; iter 390: train loss 2.04757\n",
      "iter_dt 32.45ms; iter 400: train loss 2.03819\n",
      "iter_dt 26.91ms; iter 410: train loss 2.00432\n",
      "iter_dt 30.63ms; iter 420: train loss 2.01108\n",
      "iter_dt 31.15ms; iter 430: train loss 2.01244\n",
      "iter_dt 35.49ms; iter 440: train loss 1.98116\n",
      "iter_dt 26.92ms; iter 450: train loss 1.99821\n",
      "iter_dt 30.13ms; iter 460: train loss 1.97521\n",
      "iter_dt 30.63ms; iter 470: train loss 1.96714\n",
      "iter_dt 31.84ms; iter 480: train loss 1.97938\n",
      "iter_dt 31.29ms; iter 490: train loss 1.98158\n",
      "iter_dt 26.56ms; iter 500: train loss 1.94631\n",
      "O God, O God!\n",
      "\n",
      "DORESS:\n",
      "You thou have my, agest thu ble sear.\n",
      "\n",
      "COMTHONRY BRINGELY:\n",
      "Whanclook throas but ser conted,\n",
      "Waith there and me feepare, beshy the witle,\n",
      "But my's wheere a beard wese to deare and;\n",
      "A sech why but is feice in ble that the then sly.\n",
      "\n",
      "DUKE MESMERDIA:\n",
      "I diss the son in to hy as caus it a store an\n",
      "that set an a till to the messick.\n",
      "\n",
      "BETRGBEL:\n",
      "I will soe sowetrentless, my, heereir olle her of sheirt tair haveds\n",
      "Frombencese on at be a danteoft appuram to forte a beach.\n",
      "\n",
      "PRICHAT:\n",
      "Hand, a spiell\n",
      "saving model\n",
      "iter_dt 34.07ms; iter 510: train loss 1.94618\n",
      "iter_dt 34.00ms; iter 520: train loss 1.93788\n",
      "iter_dt 32.54ms; iter 530: train loss 1.93198\n",
      "iter_dt 36.65ms; iter 540: train loss 1.91483\n",
      "iter_dt 32.99ms; iter 550: train loss 1.98481\n",
      "iter_dt 32.12ms; iter 560: train loss 1.92118\n",
      "iter_dt 27.53ms; iter 570: train loss 1.93971\n"
     ]
    }
   ],
   "source": [
    "# construct the trainer object\n",
    "trainer = Trainer(config.trainer, model, train_dataset)\n",
    "\n",
    "# iteration callback\n",
    "def batch_end_callback(trainer):\n",
    "\n",
    "    if trainer.iter_num % 10 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "    if trainer.iter_num % 500 == 0:\n",
    "        # evaluate both the train and test score\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # sample from the model...\n",
    "            context = \"O God, O God!\"\n",
    "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "            print(completion)\n",
    "        # save the latest model\n",
    "        print(\"saving model\")\n",
    "        ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        # revert model to training mode\n",
    "        model.train()\n",
    "\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# run the optimization\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AELIAN:\n",
      "We \n"
     ]
    }
   ],
   "source": [
    "model.eval();\n",
    "with torch.no_grad():\n",
    "    # create start symbol and encode it\n",
    "    context = \"Let's go\"\n",
    "    context_voc = [train_dataset.stoi[s] for s in context]\n",
    "    x = torch.tensor(context_voc , dtype=torch.long)[None,...].to(trainer.device)\n",
    "    \n",
    "    # generate predictions condition on start token\n",
    "    y = model.generate(x, 10, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "    print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 4, 11, 11, 14]\n"
     ]
    }
   ],
   "source": [
    "# Encoding visualization\n",
    "context = \"hello\"\n",
    "x = [train_dataset.stoi[s] - 39 for s in context]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opennerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
